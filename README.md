ğŸ¶ğŸ± CLIP-Based Pet Image Retriever

Fine-tuning of OpenAIâ€™s CLIP (ViT-B/32) model on the Oxford-IIIT Pet Dataset to build a text-to-image retrieval system â€” where typing â€œbulldogâ€ returns bulldog images and â€œPersian catâ€ returns Persian cats.

ğŸš€ Overview

This project fine-tunes CLIP for breed-level image retrieval of cats and dogs.
The pretrained CLIP model achieved 71.4% zero-shot accuracy,
which improved to 85.2% after fine-tuning.

ğŸ§  Fine-Tuning Details

Base model: CLIP ViT-B/32 (openai weights)

Layers fine-tuned:

Vision encoder blocks 10 & 11

Projection layer

Text encoder: kept frozen

Epochs: 10

Dataset: Oxford-IIIT Pet Dataset
 via Hugging Face datasets

Hardware: 4GB GPU compatible
<pre><code>
ğŸ§© Repository Structure
CLIP_based_image_retriever/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py               # FastAPI backend (serves /search endpoint)
â”‚   â”œâ”€â”€ retriever.py         # CLIP similarity search logic
â”‚   â”œâ”€â”€ model_loader.py      # Loads model + preprocess
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html           # Web UI for search
â”‚   â”œâ”€â”€ script.js            # Handles prompt â†’ backend call
â”‚
â”œâ”€â”€ test_data.py             # Downloads and saves Oxford-IIIT Pet test images
â”œâ”€â”€ build_index.py           # Builds CLIP vector store (image embeddings)
â”œâ”€â”€ finetune.py              # Fine-tunes CLIP model
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/              # Holds images downloaded by test_data.py
â”‚   â””â”€â”€ image_index.pt       # Saved image embeddings
â”‚
â””â”€â”€ README.md
</code></pre>

âš™ï¸ Setup
1ï¸âƒ£ Clone the repository
git clone https://github.com/<your-username>/CLIP_based_image_retriever.git
cd CLIP_based_image_retriever

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

ğŸ¾ Dataset Preparation

Download the Oxford-IIIT Pet test split and save locally:

python test_data.py


This creates a folder:
<pre><code>
data/images/
â”œâ”€â”€ 0_1.jpg
â”œâ”€â”€ 1_3.jpg
â”œâ”€â”€ 2_7.jpg
...
</code></pre>
ğŸ§® Build the Image Index

Next, encode the images using CLIP to create a searchable vector store:

python build_index.py


This generates:

data/image_index.pt

ğŸ§  Fine-Tune CLIP

Fine-tune the image encoder on pet breeds to improve retrieval accuracy:

python finetune.py


After training for 10 epochs, the accuracy improves from 71.4% â†’ 85.2%.

ğŸŒ Run the Backend

Start the FastAPI server:

uvicorn backend.app:app --reload


This serves:

/search?prompt=bulldog â†’ returns top 5 most similar images

/images/<filename>.jpg â†’ serves images statically

ğŸ’» Launch the Frontend

Open your web app (for example via a local server):

python -m http.server 5500


Then go to:

http://127.0.0.1:5500/frontend/index.html


Enter a prompt like:

"German shepherd"

and youâ€™ll see top matching dog images instantly retrieved from your fine-tuned model.

ğŸ“Š Results
Model	Accuracy	Notes
CLIP (zero-shot)	71.4%	Pretrained ViT-B/32
Fine-tuned CLIP	85.2%	Vision encoder partially fine-tuned
ğŸ§° Tech Stack

PyTorch â€” Deep learning & fine-tuning

OpenCLIP â€” CLIP model implementation

FastAPI â€” REST backend

HTML + JS â€” Simple frontend

Hugging Face Datasets â€” Data loading

Uvicorn â€” App server

ğŸ“ Required Structure Before Running
<pre><code>
data/
â”œâ”€â”€ images/          # from test_data.py
â””â”€â”€ image_index.pt   # from build_index.py 
</code></pre>

